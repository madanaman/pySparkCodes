{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /Users/amanmadan/opt/anaconda3/lib/python3.8/site-packages/IPython/utils/py3compat.py:168 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2dfc28fca47d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.1.1/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.1.1/libexec/python/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    343\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=PySparkShell, master=local[*]) created by <module> at /Users/amanmadan/opt/anaconda3/lib/python3.8/site-packages/IPython/utils/py3compat.py:168 "
     ]
    }
   ],
   "source": [
    "sc = SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD = sc.parallelize([('Mike', 19), ('June', 18), ('Rachel',16), ('Rob', 18), ('Scott', 17)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Mike', 19), ('June', 18)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRDD = (sc.textFile('/Volumes/Data\\ Disk/data_disk_workspace/pythonProjects/pySparkRecipes/data/airport-codes-na.txt' \n",
    "                     , minPartitions=4\n",
    "                    , use_unicode=True).map(lambda element: element.split(\"\\t\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['City', 'State', 'Country', 'IATA'], ['Abbotsford', 'BC', 'Canada', 'YXX']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "527"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "depatureDelaysRDD = (sc.textFile('/Volumes/Data\\ Disk/data_disk_workspace/pythonProjects/pySparkRecipes/data/departuredelays.csv'\n",
    "                                , minPartitions=4\n",
    "                                , use_unicode=True).map(lambda x: x.split(\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.881323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=datetime.now()\n",
    "depatureDelaysRDD.count()\n",
    "b=datetime.now()\n",
    "print(b-a)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depatureDelaysRDD.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "depatureDelaysRDD_nonParition = (sc.textFile('/Volumes/Data\\ Disk/data_disk_workspace/pythonProjects/pySparkRecipes/data/departuredelays.csv'\n",
    "                                , use_unicode=True).map(lambda x: x.split(\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:01.896087\n"
     ]
    }
   ],
   "source": [
    "a=datetime.now()\n",
    "depatureDelaysRDD_nonParition.count()\n",
    "b=datetime.now()\n",
    "print(b-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "depatureDelaysRDD_nonParition.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RDD Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports = (sc.textFile('/Volumes/Data\\ Disk/data_disk_workspace/pythonProjects/pySparkRecipes/data/airport-codes-na.txt')\n",
    "            .map(lambda row: row.split(\"\\t\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['City', 'State', 'Country', 'IATA'],\n",
       " ['Abbotsford', 'BC', 'Canada', 'YXX'],\n",
       " ['Aberdeen', 'SD', 'USA', 'ABR'],\n",
       " ['Abilene', 'TX', 'USA', 'ABI'],\n",
       " ['Akron', 'OH', 'USA', 'CAK']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights = (sc.textFile('/Volumes/Data\\ Disk/data_disk_workspace/pythonProjects/pySparkRecipes/data/departuredelays.csv')\n",
    "           .map(lambda row: row.split(\",\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['date', 'delay', 'distance', 'origin', 'destination'],\n",
       " ['01011245', '6', '602', 'ABE', 'ATL'],\n",
       " ['01020600', '-8', '369', 'ABE', 'DTW'],\n",
       " ['01021245', '-2', '602', 'ABE', 'ATL'],\n",
       " ['01020605', '-4', '602', 'ABE', 'ATL']]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('City', 'State'),\n",
       " ('Abbotsford', 'BC'),\n",
       " ('Aberdeen', 'SD'),\n",
       " ('Abilene', 'TX'),\n",
       " ('Akron', 'OH')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Map Transformation\n",
    "airports.map(lambda c: (c[0], c[1])).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Bellingham', 'WA'),\n",
       " ('Moses Lake', 'WA'),\n",
       " ('Pasco', 'WA'),\n",
       " ('Pullman', 'WA'),\n",
       " ('Seattle', 'WA')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "airports.map(lambda c: (c[0], c[1])).filter(lambda c: c[1]=='WA').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bellingham', 'WA', 'Moses Lake', 'WA', 'Pasco']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flat map Transformation\n",
    "airports.map(lambda c:(c[0], c[1])).filter(lambda c:c[1]==\"WA\").flatMap(lambda x: x).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Country', 'USA', 'Canada']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Distinct Transformation\n",
    "airports.map(lambda c:(c[2])).distinct().take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ABE', ('01011245', 'PA')),\n",
       " ('ABE', ('01020600', 'PA')),\n",
       " ('ABE', ('01021245', 'PA')),\n",
       " ('ABE', ('01020605', 'PA')),\n",
       " ('ABE', ('01031245', 'PA'))]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join Transformation\n",
    "flt = flights.map(lambda c: (c[3], c[0]))\n",
    "air = airports.map(lambda c: (c[3], c[1]))\n",
    "flt.join(air).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Abbotsford', 'YXX'), 1),\n",
       " (('Aberdeen', 'ABR'), 2),\n",
       " (('Abilene', 'ABI'), 3),\n",
       " (('Akron', 'CAK'), 4),\n",
       " (('Alamosa', 'ALS'), 5)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zip with index\n",
    "ac = airports.map(lambda c: (c[0], c[3]))\n",
    "ac.zipWithIndex().take(5)\n",
    "ac.zipWithIndex().filter(lambda x: x[1]>0).take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ABE', 5113), ('ACT', 392), ('ACV', 8429), ('ADQ', -254), ('AEX', 10193)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reduce by key\n",
    "(flights\n",
    " .zipWithIndex()\n",
    " .filter(lambda x: x[1]>0)\n",
    " .map(lambda row: row[0])\n",
    " .map(lambda c: (c[3], int(c[1])))\n",
    " .reduceByKey(lambda x, y: x + y)\n",
    " .take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ABE', 5113),\n",
       " ('ABI', 5128),\n",
       " ('ABQ', 64422),\n",
       " ('ABY', 1554),\n",
       " ('ACT', 392),\n",
       " ('ACV', 8429),\n",
       " ('ADQ', -254),\n",
       " ('AEX', 10193),\n",
       " ('AGS', 5003),\n",
       " ('ALB', 22362),\n",
       " ('ALO', 2866),\n",
       " ('AMA', 21979),\n",
       " ('ANC', 4948),\n",
       " ('ATL', 1151087),\n",
       " ('ATW', 8151),\n",
       " ('AUS', 108638),\n",
       " ('AVL', 5727),\n",
       " ('AVP', 2946),\n",
       " ('AZO', 233),\n",
       " ('BDL', 54662),\n",
       " ('BET', -645),\n",
       " ('BFL', 4022),\n",
       " ('BGR', 2852),\n",
       " ('BHM', 44355),\n",
       " ('BIL', 2616),\n",
       " ('BIS', 3825),\n",
       " ('BMI', 7817),\n",
       " ('BNA', 212243),\n",
       " ('BOI', 18004),\n",
       " ('BOS', 238602),\n",
       " ('BPT', 1936),\n",
       " ('BQK', 3952),\n",
       " ('BQN', 3943),\n",
       " ('BRO', 4967),\n",
       " ('BRW', 880),\n",
       " ('BTM', -138),\n",
       " ('BTR', 21989),\n",
       " ('BTV', 14755),\n",
       " ('BUF', 54309),\n",
       " ('BUR', 42241),\n",
       " ('BWI', 362845),\n",
       " ('BZN', 7226),\n",
       " ('CAE', 25686),\n",
       " ('CAK', 14749),\n",
       " ('CDC', 51),\n",
       " ('CDV', -1024),\n",
       " ('CEC', 2832),\n",
       " ('CHA', 7586),\n",
       " ('CHO', 2421),\n",
       " ('CHS', 30789)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sort by Key\n",
    "(flights\n",
    " .zipWithIndex()\n",
    " .filter(lambda x: x[1]>0)\n",
    " .map(lambda row: row[0])\n",
    " .map(lambda c: (c[3], int(c[1])))\n",
    " .reduceByKey(lambda x, y: x + y)\n",
    " .sortByKey()\n",
    " .take(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Bellingham', 'WA', 'USA', 'BLI'],\n",
       " ['Moses Lake', 'WA', 'USA', 'MWH'],\n",
       " ['Pasco', 'WA', 'USA', 'PSC'],\n",
       " ['Pullman', 'WA', 'USA', 'PUW'],\n",
       " ['Seattle', 'WA', 'USA', 'SEA'],\n",
       " ['Spokane', 'WA', 'USA', 'GEG'],\n",
       " ['Walla Walla', 'WA', 'USA', 'ALW'],\n",
       " ['Wenatchee', 'WA', 'USA', 'EAT'],\n",
       " ['Yakima', 'WA', 'USA', 'YKM'],\n",
       " ['Abbotsford', 'BC', 'Canada', 'YXX'],\n",
       " ['Anahim Lake', 'BC', 'Canada', 'YAA'],\n",
       " ['Campbell River', 'BC', 'Canada', 'YBL'],\n",
       " ['Castlegar', 'BC', 'Canada', 'YCG'],\n",
       " ['Cranbrook', 'BC', 'Canada', 'YXC'],\n",
       " ['Fort Nelson', 'BC', 'Canada', 'YYE'],\n",
       " ['Fort Saint John', 'BC', 'Canada', 'YXJ'],\n",
       " ['Kamloops', 'BC', 'Canada', 'YKA'],\n",
       " ['Kelowna', 'BC', 'Canada', 'YLW'],\n",
       " ['Nanaimo', 'BC', 'Canada', 'YCD'],\n",
       " ['Penticton', 'BC', 'Canada', 'YYF'],\n",
       " ['Port Hardy', 'BC', 'Canada', 'YZT'],\n",
       " ['Powell River', 'BC', 'Canada', 'YPW'],\n",
       " ['Prince George', 'BC', 'Canada', 'YXS'],\n",
       " ['Prince Rupert', 'BC', 'Canada', 'YPR'],\n",
       " ['Quesnel', 'BC', 'Canada', 'YQZ'],\n",
       " ['\"Sandspit, Queen Charlotte Islands\"', 'BC', 'Canada', 'YZP'],\n",
       " ['Smithers', 'BC', 'Canada', 'YYD'],\n",
       " ['Terrace', 'BC', 'Canada', 'YXT'],\n",
       " ['Vancouver', 'BC', 'Canada', 'YVR'],\n",
       " ['Victoria', 'BC', 'Canada', 'YYJ'],\n",
       " ['Williams Lake', 'BC', 'Canada', 'YWL']]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Union Transformation\n",
    "a = (\n",
    "    airports\n",
    "    .zipWithIndex()\n",
    "    .filter(lambda row: row[1] > 0)\n",
    "    .map(lambda row: row[0])\n",
    "    .filter(lambda c: c[1] == \"WA\")\n",
    ")\n",
    "b = (\n",
    "    airports\n",
    "    .zipWithIndex()\n",
    "    .filter(lambda row: row[1] > 0)\n",
    "    .map(lambda row: row[0])\n",
    "    .filter(lambda c: c[1] == \"BC\")\n",
    ")\n",
    "a.union(b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0\n",
      "0.004\n"
     ]
    }
   ],
   "source": [
    "# Reduce action\n",
    "data_reduce = sc.parallelize([1, 2, .5, .1, 5, .2], 1).reduce(lambda x,y:x/y)\n",
    "print(data_reduce)\n",
    "\n",
    "data_reduce1 = sc.parallelize([1, 2, .5, .1, 5, .2], 3).reduce(lambda x,y:x/y)\n",
    "print(data_reduce1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data frames\n",
    "flightsDF = spark.read\\\n",
    "            .options(header='true', inferSchema='true')\\\n",
    "            .csv('/Volumes/Data\\ Disk/data_disk_workspace/pythonProjects/pySparkRecipes/data/departuredelays.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightsDF.createOrReplaceTempView('flightsDF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+\n",
      "|origin|TotalDelay|\n",
      "+------+----------+\n",
      "|   ABE|      5113|\n",
      "|   ABI|      5128|\n",
      "|   ABQ|     64422|\n",
      "|   ABY|      1554|\n",
      "|   ACT|       392|\n",
      "|   ACV|      8429|\n",
      "|   ADQ|      -254|\n",
      "|   AEX|     10193|\n",
      "|   AGS|      5003|\n",
      "|   ALB|     22362|\n",
      "|   ALO|      2866|\n",
      "|   AMA|     21979|\n",
      "|   ANC|      4948|\n",
      "|   ATL|   1151087|\n",
      "|   ATW|      8151|\n",
      "|   AUS|    108638|\n",
      "|   AVL|      5727|\n",
      "|   AVP|      2946|\n",
      "|   AZO|       233|\n",
      "|   BDL|     54662|\n",
      "|   BET|      -645|\n",
      "|   BFL|      4022|\n",
      "|   BGR|      2852|\n",
      "|   BHM|     44355|\n",
      "|   BIL|      2616|\n",
      "|   BIS|      3825|\n",
      "|   BMI|      7817|\n",
      "|   BNA|    212243|\n",
      "|   BOI|     18004|\n",
      "|   BOS|    238602|\n",
      "|   BPT|      1936|\n",
      "|   BQK|      3952|\n",
      "|   BQN|      3943|\n",
      "|   BRO|      4967|\n",
      "|   BRW|       880|\n",
      "|   BTM|      -138|\n",
      "|   BTR|     21989|\n",
      "|   BTV|     14755|\n",
      "|   BUF|     54309|\n",
      "|   BUR|     42241|\n",
      "|   BWI|    362845|\n",
      "|   BZN|      7226|\n",
      "|   CAE|     25686|\n",
      "|   CAK|     14749|\n",
      "|   CDC|        51|\n",
      "|   CDV|     -1024|\n",
      "|   CEC|      2832|\n",
      "|   CHA|      7586|\n",
      "|   CHO|      2421|\n",
      "|   CHS|     30789|\n",
      "+------+----------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select origin, sum(delay) as TotalDelay from flightsDF group by origin order by origin\").show(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abstracting data with data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = sc.parallelize([\n",
    "      (1, 'MacBook Pro', 2015, '15\"', '16GB', '512GB SSD'\n",
    "        , 13.75, 9.48, 0.61, 4.02)\n",
    "    , (2, 'MacBook', 2016, '12\"', '8GB', '256GB SSD'\n",
    "        , 11.04, 7.74, 0.52, 2.03)\n",
    "    , (3, 'MacBook Air', 2016, '13.3\"', '8GB', '128GB SSD'\n",
    "        , 12.8, 8.94, 0.68, 2.96)\n",
    "    , (4, 'iMac', 2017, '27\"', '64GB', '1TB SSD'\n",
    "        , 25.6, 8.0, 20.3, 20.8)\n",
    "])\n",
    "sample_data_df = spark.createDataFrame(\n",
    "    sample_data\n",
    "    , [\n",
    "        'Id'\n",
    "        , 'Model'\n",
    "        , 'Year'\n",
    "        , 'ScreenSize'\n",
    "        , 'RAM'\n",
    "        , 'HDD'\n",
    "        , 'W'\n",
    "        , 'D'\n",
    "        , 'H'\n",
    "        , 'Weight'\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "|  1|MacBook Pro|2015|       15\"|16GB|512GB SSD|13.75|9.48|0.61|  4.02|\n",
      "|  2|    MacBook|2016|       12\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|\n",
      "|  3|MacBook Air|2016|     13.3\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|\n",
      "|  4|       iMac|2017|       27\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Id: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- Year: long (nullable = true)\n",
      " |-- ScreenSize: string (nullable = true)\n",
      " |-- RAM: string (nullable = true)\n",
      " |-- HDD: string (nullable = true)\n",
      " |-- W: double (nullable = true)\n",
      " |-- D: double (nullable = true)\n",
      " |-- H: double (nullable = true)\n",
      " |-- Weight: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample_data_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Id=1, Model='MacBook Pro', Year=2015, ScreenSize='15\"', RAM='16GB', HDD='512GB SSD', W=13.75, D=9.48, H=0.61, Weight=4.02, HDD_size='512GB', HDD_type='SSD', Volume_cuIn=80.0),\n",
       " Row(Id=2, Model='MacBook', Year=2016, ScreenSize='12\"', RAM='8GB', HDD='256GB SSD', W=11.04, D=7.74, H=0.52, Weight=2.03, HDD_size='256GB', HDD_type='SSD', Volume_cuIn=44.0)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.sql as sql\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "sample_data_transformed = (sample_data_df.rdd\n",
    "                           .map(lambda row:sql.Row(**row.asDict(), HDD_size = row.HDD.split(\" \")[0]))\n",
    "                           .map(lambda row:sql.Row(**row.asDict(), HDD_type = row.HDD.split(\" \")[1]))\n",
    "                           .map(lambda row:sql.Row(**row.asDict(), Volume=row.H * row.D * row.W))\n",
    "                           .toDF()\n",
    "                           .select (\n",
    "                           sample_data_df.columns + [\n",
    "                               'HDD_size',\n",
    "                               'HDD_type',\n",
    "                               f.round(f.col('Volume')).alias('Volume_cuIn')\n",
    "                               ])\n",
    "                        )\n",
    "sample_data_transformed.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Infering schema using reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date,delay,distance,origin,destination\n",
      "+--------+-----+--------+------+-----------+\n",
      "|    date|delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|   602.0|   ABE|        ATL|\n",
      "|01020600|   -8|   369.0|   ABE|        DTW|\n",
      "|01021245|   -2|   602.0|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql as sql\n",
    "sample_data_rdd = sc.textFile('/Volumes/Data\\ Disk/data_disk_workspace/pythonProjects/pySparkRecipes/data/departuredelays.csv')\n",
    "header = sample_data_rdd.first()\n",
    "print(header)\n",
    "sample_data_rdd_row = (sample_data_rdd\n",
    "                       .filter(lambda row: row != header)\n",
    "                       .map(lambda row: row.split(\",\"))\n",
    "                       .map(lambda row: \n",
    "                           sql.Row(\n",
    "                               date = row[0],\n",
    "                               delay = int(row[1]),\n",
    "                               distance = float(row[2]),\n",
    "                               origin = row[3],\n",
    "                               destination = row[4]\n",
    "                           ))\n",
    "                      )\n",
    "sample_data_rdd_row.take(4)\n",
    "sample_data_rdd_row_df = spark.createDataFrame(sample_data_rdd_row)\n",
    "sample_data_rdd_row_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    Date|Delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Specifiy schema programatically\n",
    "import pyspark.sql as sql\n",
    "import pyspark.sql.types as typ\n",
    "sample_data_rdd = sc.textFile('/Volumes/Data\\ Disk/data_disk_workspace/pythonProjects/pySparkRecipes/data/departuredelays.csv')\n",
    "header = sample_data_rdd.first()\n",
    "\n",
    "sch = typ.StructType([\n",
    "    typ.StructField('Date', typ.StringType(), False),\n",
    "    typ.StructField('Delay', typ.LongType(), True),\n",
    "    typ.StructField('distance', typ.LongType(), True),\n",
    "    typ.StructField('origin', typ.StringType(), True),\n",
    "    typ.StructField('destination', typ.StringType(), True)\n",
    "])\n",
    "\n",
    "sample_data_rdd = (sample_data_rdd\n",
    "                   .filter(lambda row: row!=header)\n",
    "                   .map(lambda row: row.split(\",\"))\n",
    "                   .map(lambda row:(\n",
    "                       row[0],\n",
    "                       int(row[1]),\n",
    "                       int(row[2]),\n",
    "                       row[3],\n",
    "                       row[4]\n",
    "                   ))\n",
    "                  )\n",
    "\n",
    "sample_data_with_schema = spark.createDataFrame(sample_data_rdd, schema=sch)\n",
    "sample_data_with_schema.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+\n",
      "|    Date|Delay|distance|origin|destination|\n",
      "+--------+-----+--------+------+-----------+\n",
      "|01011245|    6|     602|   ABE|        ATL|\n",
      "|01020600|   -8|     369|   ABE|        DTW|\n",
      "|01021245|   -2|     602|   ABE|        ATL|\n",
      "|01020605|   -4|     602|   ABE|        ATL|\n",
      "|01031245|   -4|     602|   ABE|        ATL|\n",
      "|01030605|    0|     602|   ABE|        ATL|\n",
      "|01041243|   10|     602|   ABE|        ATL|\n",
      "|01040605|   28|     602|   ABE|        ATL|\n",
      "|01051245|   88|     602|   ABE|        ATL|\n",
      "|01050605|    9|     602|   ABE|        ATL|\n",
      "+--------+-----+--------+------+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Create temporary view\n",
    "sample_data_with_schema.createOrReplaceTempView('sample_data_view')\n",
    "spark.sql('''\n",
    "select * from sample_data_view\n",
    "''').show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+----+----------+----+---------+-----+----+----+------+----------+\n",
      "| Id|      Model|Year|ScreenSize| RAM|      HDD|    W|   D|   H|Weight|FormFactor|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+----------+\n",
      "|  4|       iMac|2017|       27\"|64GB|  1TB SSD| 25.6| 8.0|20.3|  20.8|   Desktop|\n",
      "|  1|MacBook Pro|2015|       15\"|16GB|512GB SSD|13.75|9.48|0.61|  4.02|    Laptop|\n",
      "|  3|MacBook Air|2016|     13.3\"| 8GB|128GB SSD| 12.8|8.94|0.68|  2.96|    Laptop|\n",
      "|  2|    MacBook|2016|       12\"| 8GB|256GB SSD|11.04|7.74|0.52|  2.03|    Laptop|\n",
      "+---+-----------+----+----------+----+---------+-----+----+----+------+----------+\n",
      "\n",
      "+----------+-----------+\n",
      "|FormFactor|ComputerCnt|\n",
      "+----------+-----------+\n",
      "|    Laptop|          3|\n",
      "|   Desktop|          1|\n",
      "+----------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Using SQL to interact with Dataframes\n",
    "import pyspark.sql.types as typ\n",
    "\n",
    "sch = typ.StructType([\n",
    "      typ.StructField('Id', typ.LongType(), False)\n",
    "    , typ.StructField('Model', typ.StringType(), True)\n",
    "    , typ.StructField('Year', typ.IntegerType(), True)\n",
    "    , typ.StructField('ScreenSize', typ.StringType(), True)\n",
    "    , typ.StructField('RAM', typ.StringType(), True)\n",
    "    , typ.StructField('HDD', typ.StringType(), True)\n",
    "    , typ.StructField('W', typ.DoubleType(), True)\n",
    "    , typ.StructField('D', typ.DoubleType(), True)\n",
    "    , typ.StructField('H', typ.DoubleType(), True)\n",
    "    , typ.StructField('Weight', typ.DoubleType(), True)\n",
    "])\n",
    "sample_data_rdd = sc.textFile('/Volumes/Data\\ Disk/data_disk_workspace/pythonProjects/pySparkRecipes/data/sampleTextFile.txt')\n",
    "header = sample_data_rdd.first()\n",
    "sample_data_rdd = sample_data_rdd = (\n",
    "    sample_data_rdd\n",
    "    .filter(lambda row: row != header)\n",
    "    .map(lambda row: row.split(','))\n",
    "    .map(lambda row: (\n",
    "                int(row[0])\n",
    "                , row[1]\n",
    "                , int(row[2])\n",
    "                , row[3]\n",
    "                , row[4]\n",
    "                , row[5]\n",
    "                , float(row[6])\n",
    "                , float(row[7])\n",
    "                , float(row[8])\n",
    "                , float(row[9])\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "sample_data_schema = spark.createDataFrame(sample_data_rdd, schema=sch)\n",
    "# sample_data_schema.show()\n",
    "sample_data_schema.createOrReplaceTempView('sample_data_view')\n",
    "# spark.sql('''\n",
    "# select * from sample_data_view\n",
    "# ''').show(10)\n",
    "\n",
    "models_df = sc.parallelize([\n",
    "      ('MacBook Pro', 'Laptop')\n",
    "    , ('MacBook', 'Laptop')\n",
    "    , ('MacBook Air', 'Laptop')\n",
    "    , ('iMac', 'Desktop')\n",
    "]).toDF(['Model', 'FormFactor'])\n",
    "\n",
    "models_df.createOrReplaceTempView('models')\n",
    "\n",
    "spark.sql('''\n",
    "select a.*, b.FormFactor\n",
    "from sample_data_view as a\n",
    "left join models as b\n",
    "on a.Model == b.Model\n",
    "order by Weight desc\n",
    "''').show()\n",
    "\n",
    "#Aggregations\n",
    "spark.sql('''\n",
    "    SELECT b.FormFactor\n",
    "        , COUNT(*) AS ComputerCnt\n",
    "    FROM sample_data_view AS a\n",
    "    LEFT JOIN models AS b\n",
    "        ON a.Model == b.Model\n",
    "    GROUP BY FormFactor\n",
    "''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAtaframes transformations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
